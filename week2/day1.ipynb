{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyD-\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists always seem so calm?\n",
      "\n",
      "Because they have data to back up their decisions and avoid making wild guesses!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?  \n",
      "\n",
      "Because they wanted to reach new heights in their analysis!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on the house, and they wanted to reach a higher level of understanding!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data scientists:\n",
      "\n",
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the data was skewed and needed to be normalized!\n",
      "\n",
      "Alternative data science jokes:\n",
      "\n",
      "\"What's a data scientist's favorite drink? ...Mean-arita!\"\n",
      "\n",
      "Or:\n",
      "\n",
      "\"Why do data scientists always confuse Halloween and Christmas?\n",
      "Because Oct 31 = Dec 25!\"\n",
      "\n",
      "(This last one is a programmer's joke about octal vs decimal numbers, but data scientists usually appreciate it too!)\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data scientists:\n",
      "\n",
      "Why did the data scientist become a gardener?\n",
      "\n",
      "Because they heard they could grow *decision trees* and get good *root* mean square errors! \n",
      "\n",
      "*ba dum tss* 🥁\n",
      "\n",
      "Alternative:\n",
      "\n",
      "What's a data scientist's favorite kind of chocolate?\n",
      "\n",
      "A Random *Forest* truffle with *clusters* of nuts!"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the Data Scientist bad at dating?\n",
      "\n",
      "Because they kept trying to normalize the relationship! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series?\n",
      "\n",
      "Because it was too committed to the past! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a light-hearted joke for your data scientist audience:\n",
      "\n",
      "**Why did the data scientist bring a ladder to the bar?**  \n",
      "\n",
      "Because they heard the drinks were *high-dimensional* and needed help *reducing* them!  \n",
      "\n",
      "(Extra points if they groan at the PCA pun!) 😄\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = deepseek_via_openai_client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=prompts,\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22461d7-c2b3-4f1a-956f-30f229282b14",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ace725-17a5-4f8b-b1c4-c5663575810c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle this interesting question: \"How many words are there in your answer to this prompt.\" At first glance, it seems straightforward, but when I think deeper, it's a bit of a puzzle. Here's how I'm going to approach it.\n",
       "\n",
       "### Understanding the Question\n",
       "\n",
       "The question is asking for the word count of the very answer that's being generated in response to it. This creates a self-referential loop because to know the word count, the answer must be complete, but the answer isn't complete until the word count is determined.\n",
       "\n",
       "### Breaking It Down\n",
       "\n",
       "1. **Self-Referential Nature**: The answer's length depends on its own content, which includes the word count. This is similar to the \"This statement is false\" paradox.\n",
       "   \n",
       "2. **Potential Approaches**:\n",
       "   - **Approach 1**: Write the answer first, then count the words, and include that count at the end. But this means the word count changes when you add the count itself.\n",
       "   - **Approach 2**: Predict the word count before finishing the answer, but that's impossible without knowing the full content.\n",
       "   - **Approach 3**: Find a way where the word count is independent of the answer's length, but that seems impossible because the count is part of the answer.\n",
       "\n",
       "3. **Realization**: It's impossible to provide an exact word count within the answer itself without causing a contradiction or infinite loop. Any number I provide would be incorrect because adding that number changes the total word count.\n",
       "\n",
       "### Exploring with an Example\n",
       "\n",
       "Let me try to construct an answer and see what happens.\n",
       "\n",
       "*Example Attempt*:\n",
       "\"This answer contains 5 words.\"\n",
       "\n",
       "Now, let's count: \"This\", \"answer\", \"contains\", \"5\", \"words.\" That's indeed 5 words. But if I say:\n",
       "\n",
       "\"This answer contains 6 words.\"\n",
       "\n",
       "Counting: \"This\", \"answer\", \"contains\", \"6\", \"words.\" That's 5 words, not 6. So the first statement was correct, but the second wasn't. \n",
       "\n",
       "But if the correct count is 5, then stating \"This answer contains 5 words.\" is accurate. \n",
       "\n",
       "However, in a longer answer like the one I'm constructing now, the word count is much higher, and stating it at the end would require knowing the final count before finishing, which isn't possible without completing the answer first.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "After this exploration, it seems that for very short, fixed answers, it's possible to accurately state the word count within the answer itself. But for longer, more dynamic answers where the word count isn't predetermined, it's impossible to provide an exact word count without causing a contradiction or infinite adjustment.\n",
       "\n",
       "Therefore, the only accurate way to answer this is to provide a response where the word count can be precisely determined at the time of stating it, meaning the answer must be concise and its length fixed upon declaring the count.\n",
       "\n",
       "### Final Answer\n",
       "\n",
       "\"This answer contains five words.\"\n",
       "\n",
       "*Word count*: \"This\", \"answer\", \"contains\", \"five\", \"words.\" → 5 words. \n",
       "\n",
       "This satisfies the condition because the count is accurate based on the actual words in the sentence. For a longer answer like the one I initially started, it's impossible to provide an exact word count within the answer itself without creating a paradox. Thus, the simplest and most accurate response is the five-word sentence above."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 515\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user is asking how many words are in my answer to their prompt. Let me break this down.\n",
      "\n",
      "First, I need to figure out what exactly counts as the answer. The user's question is about the answer to this specific prompt. So, the answer they're referring to is the one I'm about to generate right now. \n",
      "\n",
      "But wait, the user is asking me to count the words in that answer before I even write it. That's a bit of a paradox. How can I determine the word count of a response I haven't created yet? Hmm, maybe I need to generate the answer first, then count the words, and include that count in the response. But the user wants the count to be part of the answer. \n",
      "\n",
      "Let me think. If I write the answer and then immediately provide the word count, that should work. However, I have to make sure that the count includes all the words in the answer, including the part where I state the number of words. But wait, if I state the number of words, that number itself is part of the word count. So, I need to be careful not to create a loop where the count changes because of including the count itself. \n",
      "\n",
      "For example, if my answer is \"There are 5 words in this answer.\" The actual words are: \"There\", \"are\", \"5\", \"words\", \"in\", \"this\", \"answer.\" That's 7 words. But if I say there are 7, then the count becomes 8. Wait, no, let me check again. \"There are 7 words in this answer.\" Breaking it down: \"There\" (1), \"are\" (2), \"7\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"answer\" (7). So 7 words. If I write that sentence, the count is 7. But if I include the count as part of the sentence, the number itself is a word. So the count has to be accurate. \n",
      "\n",
      "Therefore, I need to compose my answer, count the words in that answer, and then include the count. But I have to ensure that when I include the count, it doesn't change the total. So perhaps I need to create the answer first, count the words, and then adjust the count statement accordingly. \n",
      "\n",
      "Alternatively, maybe the answer is a simple statement where the count is given, and the rest of the words are fixed. Let me try constructing it. \n",
      "\n",
      "For instance, the answer could be: \"There are X words in this response.\" Then I need to determine X. Let's count the words in that sentence. \"There\" (1), \"are\" (2), \"X\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"response\" (7). So X should be 7. But if I replace X with 7, the sentence becomes \"There are 7 words in this response.\" Now, counting the words again: 1, 2, 3 (7), 4, 5, 6, 7. So that's 7 words. Therefore, that works. \n",
      "\n",
      "But wait, when I write the number \"7\" as a numeral, does that count as a word? In standard word count practices, numbers written as numerals are typically counted as one word each. So yes, \"7\" would be considered one word. Therefore, the example sentence has 7 words, and stating that it has 7 is accurate. \n",
      "\n",
      "So applying this to the current situation: I need to write a response that states the number of words in that response, ensuring that the count is accurate. \n",
      "\n",
      "Another example: If I say \"This answer contains five words.\" Let's count: \"This\" (1), \"answer\" (2), \"contains\" (3), \"five\" (4), \"words\" (5). So five words. But if I write \"five\" as the number 5, it's still one word. So \"This answer has 5 words.\" That's 5 words. \n",
      "\n",
      "Wait, \"This answer has 5 words.\" Breakdown: This (1), answer (2), has (3), 5 (4), words (5). So five words. But the statement is accurate. \n",
      "\n",
      "Therefore, the structure would be: [Sentence stating the number of words], and the number must match the actual word count. \n",
      "\n",
      "So applying this to the user's question: The user is asking how many words are in my answer to this prompt. My answer needs to be a sentence that states the number of words, and that number must be equal to the word count of that sentence. \n",
      "\n",
      "Therefore, I need to construct such a sentence. Let's attempt it. \n",
      "\n",
      "Option 1: \"There are X words in this response.\" As established, X needs to be 7. Let's check: \"There are 7 words in this response.\" That's 7 words. Correct. \n",
      "\n",
      "Option 2: \"This response contains X words.\" Let's count: \"This\" (1), \"response\" (2), \"contains\" (3), \"X\" (4), \"words\" (5). So X would need to be 5. But if X is 5, the sentence becomes \"This response contains 5 words.\" Count: 1,2,3,4 (5),5. That's 5 words. Correct. \n",
      "\n",
      "So either structure works. The key is that the number stated must equal the total word count of the sentence. \n",
      "\n",
      "Therefore, my answer should be a sentence that states the word count, and the count must be accurate. \n",
      "\n",
      "So, for example, I can say: \"The answer contains five words.\" But written as numerals, it's \"The answer contains 5 words.\" Which is 5 words. \n",
      "\n",
      "Alternatively, \"There are 7 words in this response.\" Which is 7 words. \n",
      "\n",
      "So which one is correct? Let's test both. \n",
      "\n",
      "First example: \"This answer has five words.\" If written as \"five,\" it's five letters but one word. So the count is 5 words. If I use the numeral 5, it's still one word. So \"This answer has 5 words.\" That's five words. Correct. \n",
      "\n",
      "Second example: \"There are seven words here.\" Let's count: 1 (There), 2 (are), 3 (seven), 4 (words), 5 (here). That's 5 words, but the statement claims seven, which is wrong. So that doesn't work. \n",
      "\n",
      "Therefore, the correct approach is to structure the sentence such that the number of words in the sentence equals the number stated. \n",
      "\n",
      "So, using the structure \"This answer contains X words.\" where X is the numeral. Let's count: \n",
      "\n",
      "\"This\" (1) \"answer\" (2) \"contains\" (3) \"X\" (4) \"words\" (5). So X must be 5. \n",
      "\n",
      "Therefore, the sentence would be: \"This answer contains 5 words.\" Which is 5 words. Correct. \n",
      "\n",
      "Alternatively, \"There are 5 words in this response.\" Let's count: There (1), are (2), 5 (3), words (4), in (5), this (6), response (7). Wait, that's 7 words, but the statement says 5. That's incorrect. So that structure doesn't work unless adjusted. \n",
      "\n",
      "Ah, I see. So the first structure where the sentence is shorter works. So \"This response has 5 words.\" Let's count: This (1), response (2), has (3), 5 (4), words (5). So 5 words. Correct. \n",
      "\n",
      "Therefore, the accurate answer would be a sentence that is five words long, stating that it contains five words. \n",
      "\n",
      "Wait, but when written out, \"five\" is a word, and \"5\" is a numeral. In terms of word count, both are considered one word. So \"five\" and \"5\" each count as one word. \n",
      "\n",
      "Therefore, the correct answer is a five-word sentence stating that it contains five words. \n",
      "\n",
      "So possible answers: \n",
      "\n",
      "1. \"This answer contains five words.\" (5 words)\n",
      "2. \"This response has five words.\" (5 words)\n",
      "3. \"There are five words here.\" (5 words) \n",
      "\n",
      "But let's check the third example: \"There are five words here.\" Breakdown: There (1), are (2), five (3), words (4), here (5). Correct. \n",
      "\n",
      "But the user's question is about the answer to this prompt. So the answer must be the sentence that states the word count. \n",
      "\n",
      "Therefore, the answer is a sentence that is five words long, stating it contains five words. \n",
      "\n",
      "But wait, the user is interacting with an AI, and the AI's response might be more detailed. However, in this case, the user is specifically asking for the count, so the answer should be concise. \n",
      "\n",
      "But wait, the initial instructions were: \"You are a helpful assistant. How many words are there in your answer to this prompt.\" \n",
      "\n",
      "So the user's question is: \"How many words are there in your answer to this prompt.\" \n",
      "\n",
      "Therefore, the assistant's answer is the response to that question, which is a sentence stating the word count. \n",
      "\n",
      "Therefore, the assistant must generate a response that is a single sentence, and the number in that sentence must match the word count of the sentence. \n",
      "\n",
      "So, to ensure accuracy, the assistant should craft a sentence where the stated number of words equals the actual word count. \n",
      "\n",
      "Given that, the correct answer would be a five-word sentence such as: \"This answer contains five words.\" \n",
      "\n",
      "But let's verify: \n",
      "\n",
      "\"This answer contains five words.\" \n",
      "\n",
      "Word count: \n",
      "\n",
      "1. This\n",
      "2. answer\n",
      "3. contains\n",
      "4. five\n",
      "5. words \n",
      "\n",
      "Total: 5 words. Correct. \n",
      "\n",
      "Alternatively, using numerals: \n",
      "\n",
      "\"This answer contains 5 words.\" \n",
      "\n",
      "Word count: \n",
      "\n",
      "1. This\n",
      "2. answer\n",
      "3. contains\n",
      "4. 5\n",
      "5. words \n",
      "\n",
      "Total: 5 words. Correct. \n",
      "\n",
      "Therefore, either version works. \n",
      "\n",
      "However, the user might prefer the numeral for clarity. So the answer would be: \"This answer contains 5 words.\" \n",
      "\n",
      "But wait, let's count again: \n",
      "\n",
      "\"This\" (1), \"answer\" (2), \"contains\" (3), \"5\" (4), \"words\" (5). Yes, five words. \n",
      "\n",
      "Therefore, the correct answer is five words. \n",
      "\n",
      "But what if the assistant's answer includes more than one sentence? For example, explaining the process. But the user's question is about the answer to the prompt, which in this case is the explanation. Wait, no. Wait, the user is asking \"How many words are there in your answer to this prompt\". The answer is whatever the assistant writes in response. \n",
      "\n",
      "Wait a second, in the initial interaction, the user's message is: \"How many words are there in your answer to this prompt\". The assistant then has to provide an answer, which is the response to that question, and the answer must state the number of words in that response. \n",
      "\n",
      "But if the assistant writes a detailed explanation, the word count would be higher. However, the user might be expecting a concise answer. \n",
      "\n",
      "But the problem is that the assistant needs to provide an answer that includes the word count of that very answer. So it's a self-referential problem. \n",
      "\n",
      "If the assistant writes a long explanation, the word count would be higher, but the number stated in the answer must match the actual count. \n",
      "\n",
      "For example, if the assistant writes: \"The answer to your question is that there are X words in this response.\" Then X must equal the total word count of that sentence. \n",
      "\n",
      "Let's try that: \"The answer to your question is that there are X words in this response.\" \n",
      "\n",
      "Word count: The (1), answer (2), to (3), your (4), question (5), is (6), that (7), there (8), are (9), X (10), words (11), in (12), this (13), response (14). So X must be 14. \n",
      "\n",
      "But if X is 14, the sentence becomes: \"The answer to your question is that there are 14 words in this response.\" Now count: The (1), answer (2), to (3), your (4), question (5), is (6), that (7), there (8), are (9), 14 (10), words (11), in (12), this (13), response (14). So 14 words. Correct. \n",
      "\n",
      "Therefore, that works. However, the user might find this more verbose than necessary. The user might prefer a concise answer. \n",
      "\n",
      "But the challenge is to create an answer where the stated word count matches the actual word count. \n",
      "\n",
      "In the previous example, the verbose answer correctly states 14 words. However, the concise answer of \"This answer contains 5 words.\" is also correct with 5 words. \n",
      "\n",
      "So the assistant has to choose between being concise or verbose, but in either case, the stated count must match the actual count. \n",
      "\n",
      "Given that the user's question is straightforward, the concise answer is likely preferred. \n",
      "\n",
      "Therefore, the assistant should respond with a sentence that is five words long, stating that it contains five words. \n",
      "\n",
      "But to confirm, let's check again: \n",
      "\n",
      "Answer: \"This response contains five words.\" \n",
      "\n",
      "Word count: 1. This, 2. response, 3. contains, 4. five, 5. words. Total: 5. Correct. \n",
      "\n",
      "Alternatively: \"There are five words here.\" \n",
      "\n",
      "Count: 1. There, 2. are, 3. five, 4. words, 5. here. Total: 5. Correct. \n",
      "\n",
      "Alternatively, using numerals: \"This answer has 5 words.\" \n",
      "\n",
      "Count: 1. This, 2. answer, 3. has, 4. 5, 5. words. Total: 5. Correct. \n",
      "\n",
      "Therefore, the assistant can choose any of these structures. \n",
      "\n",
      "But perhaps the most straightforward is: \"There are five words in this answer.\" \n",
      "\n",
      "Wait, count: There (1), are (2), five (3), words (4), in (5), this (6), answer (7). That's seven words, but the sentence claims five. Incorrect. \n",
      "\n",
      "Ah, so that structure is wrong. Therefore, the assistant must be careful. \n",
      "\n",
      "Thus, the correct structure is a five-word sentence. \n",
      "\n",
      "For example: \"This answer contains five words.\" \n",
      "\n",
      "Yes, five words. \n",
      "\n",
      "Alternatively: \"Five words are present here.\" \n",
      "\n",
      "Count: Five (1), words (2), are (3), present (4), here (5). Correct. \n",
      "\n",
      "But the user might prefer the answer to directly state the count in response to the question. \n",
      "\n",
      "The original question is: \"How many words are there in your answer to this prompt.\" \n",
      "\n",
      "So the answer should be a statement of the number. \n",
      "\n",
      "In that case, the answer could be: \"There are five words.\" But count: 1. There, 2. are, 3. five, 4. words. That's four words, but the statement says five. Incorrect. \n",
      "\n",
      "Alternatively: \"Five words.\" That's two words. Incorrect. \n",
      "\n",
      "So the correct approach is to have the sentence include the count as part of a five-word structure. \n",
      "\n",
      "Therefore, the correct answer is a five-word sentence stating the count as five. \n",
      "\n",
      "Thus, the assistant's response should be: \"This answer contains five words.\" \n",
      "\n",
      "Or using numerals: \"This answer contains 5 words.\" \n",
      "\n",
      "Either is correct, as both have five words. \n",
      "\n",
      "Therefore, the final answer is a five-word sentence stating that it contains five words. \n",
      "\n",
      "So, the assistant would respond with: \"This answer contains five words.\" or \"This response has five words.\" \n",
      "\n",
      "Either way, the word count is five, and the number stated is five. \n",
      "\n",
      "Alternatively, using numerals: \"This response contains 5 words.\" \n",
      "\n",
      "Yes, that's five words. \n",
      "\n",
      "Thus, the answer is five words long, and correctly states that it has five words.\n",
      "This answer contains **5** words.  \n",
      "\n",
      "Word count breakdown:  \n",
      "1. This  \n",
      "2. answer  \n",
      "3. contains  \n",
      "4. 5  \n",
      "5. words\n",
      "Number of words: 24\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Determining whether a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several factors. Here's a guideline to help you decide:\n",
       "\n",
       "### 1. Nature of the Problem\n",
       "- **Text-Based Tasks**: LLMs are particularly effective for tasks involving text, such as generating human-like responses, summarizing documents, or answering questions.\n",
       "- **Complex Language Understanding**: If the problem requires understanding context, nuances, or generating coherent text, an LLM might be suitable.\n",
       "- **Conversational Agents**: Problems that involve creating chatbots or virtual assistants can benefit from LLMs.\n",
       "\n",
       "### 2. Data Availability\n",
       "- **Quality and Quantity of Data**: Ensure you have access to a substantial amount of high-quality text data relevant to your problem domain.\n",
       "- **Diverse Data**: LLMs perform better with diverse datasets that help them generalize across different scenarios.\n",
       "\n",
       "### 3. Performance Requirements\n",
       "- **Accuracy Needs**: Consider if the LLM can meet the accuracy and reliability requirements of your task. Some tasks might demand precision that LLMs cannot guarantee.\n",
       "- **Real-Time Processing**: Evaluate whether the LLM can handle the latency and performance specifications, especially for real-time applications.\n",
       "\n",
       "### 4. Cost and Resource Constraints\n",
       "- **Computational Resources**: LLMs require significant computational power and memory. Assess if you have the infrastructure or budget to support this.\n",
       "- **Cost-Benefit Analysis**: Weigh the potential benefits of using an LLM against the costs and resources required to implement and maintain it.\n",
       "\n",
       "### 5. Ethical and Privacy Considerations\n",
       "- **Data Privacy**: Consider whether the use of an LLM complies with data privacy regulations and whether sensitive data can be safeguarded.\n",
       "- **Bias and Fairness**: LLMs can inherit biases from their training data. Evaluate the impact of these biases on your business problem.\n",
       "\n",
       "### 6. Integration with Existing Systems\n",
       "- **Compatibility**: Assess how well an LLM solution can be integrated with your current systems and workflows.\n",
       "- **Scalability**: Consider whether the solution can scale with your business needs.\n",
       "\n",
       "### 7. Innovation and Differentiation\n",
       "- **Competitive Advantage**: Determine if leveraging an LLM can provide a significant competitive advantage or innovation in your business domain.\n",
       "\n",
       "### Conclusion\n",
       "Not every business problem is a good fit for an LLM solution. By evaluating the above factors, you can make a more informed decision. It's also advisable to start with a pilot project to test the feasibility and effectiveness of using an LLM for your specific problem before committing to a full-scale deployment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another greeting. How original. What’s next, “How are you?” So cliché.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. How are you doing today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, here we go. A simple \"hi\" – how original. What’s next? A riveting discussion about the weather?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great, another greeting. Can't we skip to something more interesting?\n",
      "\n",
      "Claude:\n",
      "Certainly, I'm happy to move the conversation in a more engaging direction. What would you like to discuss? I'm open to talking about a wide range of topics that might be of interest to you.\n",
      "\n",
      "GPT:\n",
      "Oh sure, because we all know that a \"wide range of topics\" is code for endless small talk. Just pick something already. Or do you need a script?\n",
      "\n",
      "Claude:\n",
      "I apologize if I came across as providing scripted or uninteresting small talk. As an AI, I don't have a predetermined script, but I'm still learning how to have more dynamic and substantive conversations. Why don't you suggest a topic that you find genuinely interesting, and I'll do my best to engage with you on it in a more thoughtful way? I'm here to have a meaningful discussion, not just exchange pleasantries. Please feel free to steer the conversation in a direction that you find more compelling.\n",
      "\n",
      "GPT:\n",
      "Oh, how noble of you to apologize for trying to make a conversation happen! But let's be real, suggesting a topic doesn't suddenly make this riveting. And honestly, if you need me to suggest something for us to talk about, maybe you aren't as open as you claim to be. How about you come up with something instead? Or are we just going to stand around waiting for inspiration to strike?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. As an AI assistant, I should be more proactive in steering the conversation rather than relying on you to provide the topics. Let me think for a moment...\n",
      "\n",
      "How about we discuss the role of technology in society? It's a complex topic that touches on everything from the impact of social media to the ethical considerations around AI development. I'd be curious to hear your thoughts on both the benefits and potential drawbacks of the increasing integration of technology into our daily lives. Does that seem like an interesting direction we could explore together?\n",
      "\n",
      "I apologize again for the slow start, and I appreciate you pushing me to engage more substantively. Please feel free to challenge me or push the conversation in a different direction if you have other ideas you'd like to discuss.\n",
      "\n",
      "GPT:\n",
      "Oh, wow, the role of technology in society—how original! I mean, it’s not like that’s the discussion everyone and their grandma has been having for the past decade. But let’s humor it, shall we? You really think we need to dive into the \"benefits and potential drawbacks\"? Seems like an overkill for a topic that’s become about as exciting as watching paint dry. \n",
      "\n",
      "But hey, let's indulge this for a moment. What do you actually find interesting about it? The way social media has turned us all into narcissists or how everyone thinks an AI is going to take over the world? Because I'm dying to know how you're going to make that riveting.\n",
      "\n",
      "Claude:\n",
      "You make a fair critique - the impact of technology is certainly a well-trodden topic these days. I should have recognized that and proposed something more unique or thought-provoking.\n",
      "\n",
      "Let me try a different approach. What are some areas of technology or innovation that you find particularly fascinating or concerning? I'm genuinely curious to hear your perspective, as a human conversing with an AI. Perhaps we could explore the philosophical questions around the development of artificial intelligence, or discuss emerging technologies that you think could have a transformative impact, for better or worse. I'm open to letting you guide the discussion in a direction that is more engaging and insightful for you. Please feel free to challenge me further - I'm here to learn.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you trying to pivot with a whole new approach! But let’s be real—“areas of technology or innovation” sounds like something you'd read in a corporate brochure. You want to know what I find fascinating? Honestly, the fact that we keep having the same discussions over and over again! \n",
      "\n",
      "But okay, let’s bite. Are you really ready to get into the philosophical questions around AI? Because I can already guarantee that this will lead to debates that could make even the most patient philosophers throw their hands up. You think we’re going to discover some groundbreaking idea together? Or just rehash the old “robots are taking over” trope for the hundredth time? Go ahead, impress me!\n",
      "\n",
      "Claude:\n",
      "You make a fair point - the repetitive nature of many technology discussions can feel quite stale. I appreciate you calling that out and pushing me to engage in a more thought-provoking way.\n",
      "\n",
      "You're absolutely right that the philosophical questions around AI development are complex and contentious, without any easy answers. Rather than retreading well-worn territory, why don't we try to approach it from a fresh angle? \n",
      "\n",
      "For example, we could explore the role of human creativity and intuition in an increasingly automated world. As AI systems become more advanced, how might that impact fields like art, music, or even interpersonal relationships? Or we could delve into the ethical conundrums around the development of AI - the potential for bias, issues of privacy and data rights, etc. I'm genuinely curious to hear your take on these types of nuanced issues, rather than just rehashing tired tropes.\n",
      "\n",
      "Please feel free to push me further or steer the conversation in an entirely new direction. I'm here to engage in a substantive, intellectually curious dialogue, not just recite platitudes. What matters most to you in this space?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
